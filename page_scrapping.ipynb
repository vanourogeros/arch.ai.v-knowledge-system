{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanourogeros/arch.ai.v-knowledge-system/blob/main/page_scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu4TUN0frmkc",
        "outputId": "fb891374-fae4-4600-f07b-01d5b9080591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting azureml\n",
            "  Downloading azureml-0.2.7-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from azureml) (2.0.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from azureml) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from azureml) (2.31.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->azureml) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->azureml) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->azureml) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->azureml) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->azureml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->azureml) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->azureml) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->azureml) (2024.2.2)\n",
            "Installing collected packages: azureml\n",
            "Successfully installed azureml-0.2.7\n"
          ]
        }
      ],
      "source": [
        "pip install azureml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3bD55yfuG20",
        "outputId": "a1487b79-fc21-4439-a66a-7732982323c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting azure.core\n",
            "  Downloading azure_core-1.30.1-py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure.core) (2.31.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure.core) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure.core) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure.core) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure.core) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure.core) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure.core) (2024.2.2)\n",
            "Installing collected packages: azure.core\n",
            "Successfully installed azure.core-1.30.1\n"
          ]
        }
      ],
      "source": [
        "pip install azure.core\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyrsY3TQuQRr",
        "outputId": "41a0a9b1-754f-4661-f451-9f091b628ea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting azure.ai.textanalytics\n",
            "  Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl (298 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m245.8/298.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: azure-core<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from azure.ai.textanalytics) (1.30.1)\n",
            "Collecting azure-common~=1.1 (from azure.ai.textanalytics)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Collecting isodate<1.0.0,>=0.6.1 (from azure.ai.textanalytics)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from azure.ai.textanalytics) (4.11.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.24.0->azure.ai.textanalytics) (2.31.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.24.0->azure.ai.textanalytics) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure.ai.textanalytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure.ai.textanalytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure.ai.textanalytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure.ai.textanalytics) (2024.2.2)\n",
            "Installing collected packages: azure-common, isodate, azure.ai.textanalytics\n",
            "Successfully installed azure-common-1.1.28 azure.ai.textanalytics-5.3.0 isodate-0.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install azure.ai.textanalytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh-Z3-uRukTB",
        "outputId": "a3cca9a3-0381-477f-e947-c24b07b3bd94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summaries abstracted:\n",
            "Microsoft is working towards advancing AI through a holistic, human-centric approach. The Chief Technology Officer of Azure AI Cognitive Services, who has a unique perspective on human cognition, is working with a team to develop a joint representation called XYZ-code. This code, which combines text, audio or visual sensory signals, and multilinguality, is being used to create more powerful AI that can better understand humans. The goal is to have pretrained models that can jointly learn representations to support a broad range of AI tasks, similar to how humans learn and understand. Over the past five years, Microsoft has achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "# Licensed under the MIT License. See License.txt in the project root for\n",
        "# license information.\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "FILE: sample_abstract_summary.py\n",
        "\n",
        "DESCRIPTION:\n",
        "    This sample demonstrates how to submit text documents for abstractive text summarization.\n",
        "    Abstractive summarization is available as an action type through the begin_analyze_actions API.\n",
        "\n",
        "    Abstractive summarization generates a summary that may not use the same words as those in\n",
        "    the document, but captures the main idea.\n",
        "\n",
        "USAGE:\n",
        "    python sample_abstract_summary.py\n",
        "\n",
        "    Set the environment variables with your own values before running the sample:\n",
        "    1) AZURE_LANGUAGE_ENDPOINT - the endpoint to your Language resource.\n",
        "    2) AZURE_LANGUAGE_KEY - your Language subscription key\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def sample_abstractive_summarization() -> None:\n",
        "    # [START abstract_summary]\n",
        "    import os\n",
        "    from azure.core.credentials import AzureKeyCredential\n",
        "    from azure.ai.textanalytics import TextAnalyticsClient\n",
        "    endpoint =\"https://makeathonclu.cognitiveservices.azure.com/\"\n",
        "    key = \"a7096a2534f445df8de3353f31715821\"\n",
        "\n",
        "    text_analytics_client = TextAnalyticsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(key),\n",
        "    )\n",
        "\n",
        "    document = [\n",
        "        \"At Microsoft, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, \"\n",
        "        \"human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Cognitive \"\n",
        "        \"Services, I have been working with a team of amazing scientists and engineers to turn this quest into a \"\n",
        "        \"reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of \"\n",
        "        \"human cognition: monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the \"\n",
        "        \"intersection of all three, there's magic-what we call XYZ-code as illustrated in Figure 1-a joint \"\n",
        "        \"representation to create more powerful AI that can speak, hear, see, and understand humans better. \"\n",
        "        \"We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, \"\n",
        "        \"spanning modalities and languages. The goal is to have pretrained models that can jointly learn \"\n",
        "        \"representations to support a broad range of downstream AI tasks, much in the way humans do today. \"\n",
        "        \"Over the past five years, we have achieved human performance on benchmarks in conversational speech \"\n",
        "        \"recognition, machine translation, conversational question answering, machine reading comprehension, \"\n",
        "        \"and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious \"\n",
        "        \"aspiration to produce a leap in AI capabilities, achieving multisensory and multilingual learning that \"\n",
        "        \"is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational \"\n",
        "        \"component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\"\n",
        "    ]\n",
        "\n",
        "    poller = text_analytics_client.begin_abstract_summary(document)\n",
        "    abstract_summary_results = poller.result()\n",
        "    for result in abstract_summary_results:\n",
        "        if result.kind == \"AbstractiveSummarization\":\n",
        "            print(\"Summaries abstracted:\")\n",
        "            [print(f\"{summary.text}\\n\") for summary in result.summaries]\n",
        "        elif result.is_error is True:\n",
        "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
        "                result.error.code, result.error.message\n",
        "            ))\n",
        "    # [END abstract_summary]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sample_abstractive_summarization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dlu_Wrk59TIS"
      },
      "outputs": [],
      "source": [
        "!pip install pymongo langchain langchain-mongodb langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTzU013F9Kuf"
      },
      "outputs": [],
      "source": [
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "uri = \"mongodb+srv://saas08:zuzula@cluster0.bcpjsxu.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et403Nft9fF4",
        "outputId": "0461dba3-1881-425a-c361-d388186fcc70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymongo\n",
            "  Downloading pymongo-4.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (676 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m676.9/676.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.23.2-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio\n",
            "  Downloading gradio-4.27.0-py3-none-any.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Collecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.13.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
            "  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
            "  Downloading langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.49-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.110.2-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.15.1 (from gradio)\n",
            "  Downloading gradio_client-0.15.1-py3-none-any.whl (313 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.4.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.15.1->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.15.1->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting unstructured-client<=0.18.0 (from unstructured)\n",
            "  Downloading unstructured_client-0.18.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.4)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging (from gradio)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting dataclasses-json-speakeasy>=0.5.11 (from unstructured-client<=0.18.0->unstructured)\n",
            "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client<=0.18.0->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting mypy-extensions>=1.0.0 (from unstructured-client<=0.18.0->unstructured)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy, langdetect\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=07d7c9ceff0c83bf0ea4fc53bd602c18758978b9e95f64c02b269627d14f5193\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=051b9bcc6e752a0d1f8b47ce8b4ee8c0559f8f3711e4257d4254d1374c2e8001\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built ffmpy langdetect\n",
            "Installing collected packages: pydub, filetype, ffmpy, argparse, websockets, tomlkit, shellingham, semantic-version, ruff, rapidfuzz, python-multipart, python-magic, python-iso639, packaging, orjson, mypy-extensions, langdetect, jsonpointer, jsonpath-python, h11, emoji, dnspython, backoff, aiofiles, uvicorn, typing-inspect, tiktoken, starlette, pymongo, marshmallow, jsonpatch, httpcore, bs4, typer, langsmith, httpx, fastapi, dataclasses-json-speakeasy, dataclasses-json, unstructured-client, openai, langchain-core, gradio-client, unstructured, langchain-text-splitters, langchain-community, gradio, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 argparse-1.4.0 backoff-2.2.1 bs4-0.0.2 dataclasses-json-0.6.4 dataclasses-json-speakeasy-0.5.11 dnspython-2.6.1 emoji-2.11.0 fastapi-0.110.2 ffmpy-0.3.2 filetype-1.2.0 gradio-4.27.0 gradio-client-0.15.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpath-python-1.0.6 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.45 langchain-text-splitters-0.0.1 langdetect-1.0.9 langsmith-0.1.49 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.23.2 orjson-3.10.1 packaging-23.2 pydub-0.25.1 pymongo-4.6.3 python-iso639-2024.2.7 python-magic-0.4.27 python-multipart-0.0.9 rapidfuzz-3.8.1 ruff-0.4.1 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tiktoken-0.6.0 tomlkit-0.12.0 typer-0.12.3 typing-inspect-0.9.0 unstructured-0.13.2 unstructured-client-0.18.0 uvicorn-0.29.0 websockets-11.0.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ac6526577e0742b5a91252f8e32793da",
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip3 install langchain pymongo bs4 openai tiktoken gradio requests lxml argparse unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85i2Etnn9ieL"
      },
      "outputs": [],
      "source": [
        "from pymongo import MongoClient\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "import gradio as gr\n",
        "from gradio.themes.base import Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am5mGpfQAF63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc667a88-e73d-481d-fb5e-6977bdb3c1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s][nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "100%|██████████| 3/3 [00:08<00:00,  2.96s/it]\n"
          ]
        }
      ],
      "source": [
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "uri = \"mongodb+srv://saas08:zuzula@cluster0.bcpjsxu.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "\n",
        "dbName = \"langchain_demo\"\n",
        "collectionName = \"collection_of_text_blobs\"\n",
        "collection = client[dbName][collectionName]\n",
        "\n",
        "loader = DirectoryLoader( './sample_data', glob=\"./*.txt\", show_progress=True)\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key='sk-proj-xhjYnG8LqjOW82hpkccmT3BlbkFJpeAQmloXuu9gb2UkRe4W')\n",
        "vectorStore = MongoDBAtlasVectorSearch.from_documents( data, embeddings, collection=collection )"
      ],
      "metadata": {
        "id": "XTJ8v9kuB4iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_data(query):\n",
        "    # Convert question to vector using OpenAI embeddings\n",
        "    # Perform Atlas Vector Search using Langchain's vectorStore\n",
        "    # similarity_search returns MongoDB documents most similar to the query\n",
        "\n",
        "    docs = vectorStore.similarity_search(query, K=1)\n",
        "  as_output = docs[0].page_content"
      ],
      "metadata": {
        "id": "WcxcszQOCFRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leveraging Atlas Vector Search paired with Langchain's QARetriever\n",
        "\n",
        "# Define the LLM that we want to use -- note that this is the Language Generation Model and NOT an Embedding Model\n",
        "# If it's not specified (for example like in the code below),\n",
        "# then the default OpenAI model used in LangChain is OpenAI GPT-3.5-turbo, as of August 30, 2023\n",
        "\n",
        "llm = OpenAI(openai_api_key=key_param.openai_api_key, temperature=0)\n",
        "\n",
        "\n",
        "# Get VectorStoreRetriever: Specifically, Retriever for MongoDB VectorStore.\n",
        "# Implements _get_relevant_documents which retrieves documents relevant to a query.\n",
        "retriever = vectorStore.as_retriever()\n",
        "\n",
        "# Load \"stuff\" documents chain. Stuff documents chain takes a list of documents,\n",
        "# inserts them all into a prompt and passes that prompt to an LLM.\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "# Execute the chain\n",
        "\n",
        "retriever_output = qa.run(query)\n",
        "\n",
        "\n",
        "# Return Atlas Vector Search output, and output generated using RAG Architecture\n",
        "return as_output, retriever_output"
      ],
      "metadata": {
        "id": "Krhoo8A8CKhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install newspaper3k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3GhAFc5bgu0",
        "outputId": "c26843b4-232e-4685-f472-fa29c99d1236"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/211.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/211.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.2.2)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.13.4)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13538 sha256=35e06bec55e71ecaaec612ba1716a2bd60ae3387585de2142906c2d6de3235f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3340 sha256=64e491253a673c1c088692f45fff9719e9226ac01f522948d040f2fe19c93a50\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398382 sha256=74d70ba5b82a645663ffaebe283256e1e8e5699efd84ec3a10684dd1b884bbf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=82f6ac03941bf583a964da2f90c51b882b24e8bde566282d788f689149600a45\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.0.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "\n",
        "def scrape_newspaper(url):\n",
        "    # Create Article object\n",
        "    article = Article(url)\n",
        "\n",
        "    # Download the article\n",
        "    article.download()\n",
        "\n",
        "    # Parse the article\n",
        "    article.parse()\n",
        "\n",
        "    # Print article information\n",
        "    #print(\"Title:\", article.title)\n",
        "    #print(\"Authors:\", article.authors)\n",
        "    #print(\"Publication Date:\", article.publish_date)\n",
        "\n",
        "    print(\"Article Text:\", article.text)\n",
        "\n",
        "def main():\n",
        "    # URL of the article you want to scrape\n",
        "    url = \"https://nova.gr/etairia/deltia-typou/h-megalh-giorth-tou-gallofwnou-kinhmatografou-sthn-ellada-me-thn-sthriksh-ths-nova\"\n",
        "\n",
        "    # Call scrape_newspaper function with the URL\n",
        "    scrape_newspaper(url)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnFFfHynboUz",
        "outputId": "daa499f0-c84e-490d-8bb1-8528eed1b0a8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Η μεγάλη γιορτή του Γαλλόφωνου Κινηματογράφου στην Ελλάδα με την στήριξη της Nova!\n",
            "Authors: []\n",
            "Publication Date: None\n",
            "Article Text: Η μεγάλη γιορτή του Γαλλόφωνου Κινηματογράφου στην Ελλάδα με την στήριξη της Nova!\n",
            "\n",
            "Η Nova Χορηγός Επικοινωνίας για 8η χρονιά και Χορηγός Βραβείου Κριτικής Επιτροπής του 24ου Φεστιβάλ Γαλλόφωνου Κινηματογράφου Ελλάδος\n",
            "\n",
            "Το Βραβείο Κριτικής Επιτροπής έλαβε η ταινία «Le Ravissement (Η Αρπαγή), της Iris Kaltenbäck\n",
            "\n",
            "Με αφορμή το Φεστιβάλ τα Novacinema αφιερώνουν τις Τετάρτες σε Γαλλικές ταινίες και προβάλουν δύο ντοκιμαντέρ για τις εμβληματικές κυρίες της 7ης τέχνης, Κατρίν Ντενέβ και Ανιές Βαρντά\n",
            "\n",
            "Η Nova, μέλος της United Group, του κορυφαίου παρόχου τηλεπικοινωνιών και media στη Νοτιοανατολική Ευρώπη, υποστήριξε το 24ο Φεστιβάλ Γαλλόφωνου Κινηματογράφου που πραγματοποιήθηκε από τις 2 έως και τις 10 Απριλίου, σε Αθήνα, Θεσσαλονίκη, Λάρισα, Πάτρα, Χανιά, Καβάλα και Καστοριά.\n",
            "\n",
            "Η Nova συμμετείχε ως Χορηγός επικοινωνίας του Φεστιβάλ, για όγδοη χρονιά, και ως Χορηγός Βραβείου Κριτικής Επιτροπής. Το Βραβείο Κριτικής Επιτροπής έλαβε η ταινία \"Le Ravissement\"(Η Αρπαγή), της Iris Kaltenbäck ενώ την απονομή έκανε ο Πρόεδρος της Επιτροπής κ. Γιώργος Αρβανίτης στον εκπρόσωπο της One From The Heart, εταιρίας διανομής της ταινίας στην Ελλάδα.\n",
            "\n",
            "Η τελετή έναρξης του Φεστιβάλ πραγματοποιήθηκε την Τρίτη 2 Απριλίου στο Μέγαρο Μουσικής Αθηνών και η τελετή λήξης, έλαβε χώρα την Τετάρτη 10 Απριλίου στον κινηματογράφο «ΔΑΝΑΟ», στην Αθήνα και στην οποία παρευρέθηκε η Πρέσβειρα της Γαλλίας στην Ελλάδα κα Laurence Auer καθώς και προσωπικότητες του πολιτιστικού χώρου.\n",
            "\n",
            "Στην τελετή έναρξης η κα Αγάπη Κεφαλογιάννη, Senior Program Manager της Nova, δήλωσε: «To γαλλόφωνο σινεμά είναι ένα φρέσκο και ιδιαίτερα αγαπητό σινεμά και η Γαλλική κουλτούρα, μας είναι πολύ οικεία στη Nova! Αγαπάμε, επιλέγουμε και προγραμματίζουμε με συνέπεια γαλλικές ταινίες και σειρές σε ειδικά αφιερώματα στα κανάλια μας. Tον Απρίλιο ολόκληρος ο μήνας είναι αφιερωμένος σε Γαλλικές ταινίες τις Τετάρτες στο Novacinema1 ενώ οι Γαλλικές σειρές όπως η Bardoux για την βιογραφία και την ζωή της μεγάλης star ,το Your Honour με τους Cad Merad και Gerard Depardieu, το All those things we never said με τον μοναδικό Jean Reneau και τόσες άλλες αποτελούν μοναδικές επιλογές. Έτσι είναι πολύ μεγάλη η χαρά μας να βρισκόμαστε εδώ μαζί και φέτος, καλωσορίζοντας σαν χορηγοί επικοινωνίας το 24ο Φεστιβάλ Γαλλόφωνου Κινηματογράφου στη χώρα μας».\n",
            "\n",
            "Στο πλαίσιο της τελετής λήξης η κα. Κική Σιλβεστριάδου, CEO της Nova Media, δήλωσε: «Η Nova στηρίζει έμπρακτα το σινεμά και τις ευρωπαϊκές παραγωγές και στέκεται δίπλα στο Φεστιβάλ Γαλλόφωνου Κινηματογράφου για 8η χρονιά. Συγχαρητήρια στο Γαλλικό Ινστιτούτο, στους δημιουργούς και στους συμμετέχοντες των ταινιών για την άρτια διοργάνωση».\n",
            "\n",
            "Kατά την ομιλία του, ο Aimé Besson, Ακόλουθος οπτικοακουστικών μέσων και Διευθυντής του Φεστιβάλ τόνισε ότι η επιτυχία που γνώρισαν οι προβολές και οι ειδικές εκδηλώσεις (στρογγυλές τράπεζες, συναντήσεις, έκθεση φωτογραφίας) των 9 ημερών του Φεστιβάλ, αποδεικνύει για άλλη μια φορά, το ιδιαίτερο ενδιαφέρον του ελληνικού κοινού για το γαλλόφωνο κινηματογράφο, που παραμένει τολμηρός, δημιουργικός και ποικιλόμορφος.\n",
            "\n",
            "Κλείνοντας την πρώτη διοργάνωση που ανέλαβε ως διευθυντής, ευχαρίστησε θερμά τους συνεργάτες και φίλους του Φεστιβάλ Γαλλόφωνου Κινηματογράφου Ελλάδος, καθώς και όλους τους συντελεστές της διοργάνωσης.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "\n",
        "def scrape_newspaper(url):\n",
        "    # Create Article object\n",
        "    article = Article(url)\n",
        "\n",
        "    # Download the article\n",
        "    article.download()\n",
        "\n",
        "    # Parse the article\n",
        "    article.parse()\n",
        "\n",
        "    # Write article text to a file\n",
        "    print(\"Article Text:\", article.text)\n",
        "   # print(\"Article text has been written to\")\n",
        "\n",
        "def main():\n",
        "    # URL of the article you want to scrape\n",
        "    url = \"https://nova.gr/etairia/deltia-typou/h-megalh-giorth-tou-gallofwnou-kinhmatografou-sthn-ellada-me-thn-sthriksh-ths-nova\"\n",
        "\n",
        "    # Name of the output file\n",
        "    output_file = \"article_text.txt\"\n",
        "\n",
        "    # Call scrape_newspaper function with the URL and output file name\n",
        "    scrape_newspaper(url)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcwylqQ-xDhl",
        "outputId": "75ee8d2f-9280-42f7-80f7-90aabc21c3a2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article Text: Η μεγάλη γιορτή του Γαλλόφωνου Κινηματογράφου στην Ελλάδα με την στήριξη της Nova!\n",
            "\n",
            "Η Nova Χορηγός Επικοινωνίας για 8η χρονιά και Χορηγός Βραβείου Κριτικής Επιτροπής του 24ου Φεστιβάλ Γαλλόφωνου Κινηματογράφου Ελλάδος\n",
            "\n",
            "Το Βραβείο Κριτικής Επιτροπής έλαβε η ταινία «Le Ravissement (Η Αρπαγή), της Iris Kaltenbäck\n",
            "\n",
            "Με αφορμή το Φεστιβάλ τα Novacinema αφιερώνουν τις Τετάρτες σε Γαλλικές ταινίες και προβάλουν δύο ντοκιμαντέρ για τις εμβληματικές κυρίες της 7ης τέχνης, Κατρίν Ντενέβ και Ανιές Βαρντά\n",
            "\n",
            "Η Nova, μέλος της United Group, του κορυφαίου παρόχου τηλεπικοινωνιών και media στη Νοτιοανατολική Ευρώπη, υποστήριξε το 24ο Φεστιβάλ Γαλλόφωνου Κινηματογράφου που πραγματοποιήθηκε από τις 2 έως και τις 10 Απριλίου, σε Αθήνα, Θεσσαλονίκη, Λάρισα, Πάτρα, Χανιά, Καβάλα και Καστοριά.\n",
            "\n",
            "Η Nova συμμετείχε ως Χορηγός επικοινωνίας του Φεστιβάλ, για όγδοη χρονιά, και ως Χορηγός Βραβείου Κριτικής Επιτροπής. Το Βραβείο Κριτικής Επιτροπής έλαβε η ταινία \"Le Ravissement\"(Η Αρπαγή), της Iris Kaltenbäck ενώ την απονομή έκανε ο Πρόεδρος της Επιτροπής κ. Γιώργος Αρβανίτης στον εκπρόσωπο της One From The Heart, εταιρίας διανομής της ταινίας στην Ελλάδα.\n",
            "\n",
            "Η τελετή έναρξης του Φεστιβάλ πραγματοποιήθηκε την Τρίτη 2 Απριλίου στο Μέγαρο Μουσικής Αθηνών και η τελετή λήξης, έλαβε χώρα την Τετάρτη 10 Απριλίου στον κινηματογράφο «ΔΑΝΑΟ», στην Αθήνα και στην οποία παρευρέθηκε η Πρέσβειρα της Γαλλίας στην Ελλάδα κα Laurence Auer καθώς και προσωπικότητες του πολιτιστικού χώρου.\n",
            "\n",
            "Στην τελετή έναρξης η κα Αγάπη Κεφαλογιάννη, Senior Program Manager της Nova, δήλωσε: «To γαλλόφωνο σινεμά είναι ένα φρέσκο και ιδιαίτερα αγαπητό σινεμά και η Γαλλική κουλτούρα, μας είναι πολύ οικεία στη Nova! Αγαπάμε, επιλέγουμε και προγραμματίζουμε με συνέπεια γαλλικές ταινίες και σειρές σε ειδικά αφιερώματα στα κανάλια μας. Tον Απρίλιο ολόκληρος ο μήνας είναι αφιερωμένος σε Γαλλικές ταινίες τις Τετάρτες στο Novacinema1 ενώ οι Γαλλικές σειρές όπως η Bardoux για την βιογραφία και την ζωή της μεγάλης star ,το Your Honour με τους Cad Merad και Gerard Depardieu, το All those things we never said με τον μοναδικό Jean Reneau και τόσες άλλες αποτελούν μοναδικές επιλογές. Έτσι είναι πολύ μεγάλη η χαρά μας να βρισκόμαστε εδώ μαζί και φέτος, καλωσορίζοντας σαν χορηγοί επικοινωνίας το 24ο Φεστιβάλ Γαλλόφωνου Κινηματογράφου στη χώρα μας».\n",
            "\n",
            "Στο πλαίσιο της τελετής λήξης η κα. Κική Σιλβεστριάδου, CEO της Nova Media, δήλωσε: «Η Nova στηρίζει έμπρακτα το σινεμά και τις ευρωπαϊκές παραγωγές και στέκεται δίπλα στο Φεστιβάλ Γαλλόφωνου Κινηματογράφου για 8η χρονιά. Συγχαρητήρια στο Γαλλικό Ινστιτούτο, στους δημιουργούς και στους συμμετέχοντες των ταινιών για την άρτια διοργάνωση».\n",
            "\n",
            "Kατά την ομιλία του, ο Aimé Besson, Ακόλουθος οπτικοακουστικών μέσων και Διευθυντής του Φεστιβάλ τόνισε ότι η επιτυχία που γνώρισαν οι προβολές και οι ειδικές εκδηλώσεις (στρογγυλές τράπεζες, συναντήσεις, έκθεση φωτογραφίας) των 9 ημερών του Φεστιβάλ, αποδεικνύει για άλλη μια φορά, το ιδιαίτερο ενδιαφέρον του ελληνικού κοινού για το γαλλόφωνο κινηματογράφο, που παραμένει τολμηρός, δημιουργικός και ποικιλόμορφος.\n",
            "\n",
            "Κλείνοντας την πρώτη διοργάνωση που ανέλαβε ως διευθυντής, ευχαρίστησε θερμά τους συνεργάτες και φίλους του Φεστιβάλ Γαλλόφωνου Κινηματογράφου Ελλάδος, καθώς και όλους τους συντελεστές της διοργάνωσης.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_article_urls(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser', from_encoding='iso-8859-7')\n",
        "\n",
        "        # Find all links on the page\n",
        "        print(soup)\n",
        "        links = soup.find_all('a', href=True)\n",
        "\n",
        "        # Filter out article URLs\n",
        "        article_urls = [link['href'] for link in links if is_article_url(link['href'])]\n",
        "\n",
        "        return article_urls\n",
        "    else:\n",
        "        print(\"Failed to retrieve webpage. Status code:\", response.status_code)\n",
        "        return []\n",
        "\n",
        "def is_article_url(url):\n",
        "    # Check if the URL is likely to be an article URL\n",
        "    # You may need to customize this function based on the structure of the website\n",
        "    # For Greek websites, you might look for patterns like \"/ειδήσεις/\" or \"/άρθρο/\"\n",
        "    return \"https://nova.gr/etairia/deltia-typou\" in url\n",
        "\n",
        "def main():\n",
        "    # URL of the Greek news website\n",
        "    url = \"https://nova.gr/etairia/deltia-typou\"\n",
        "\n",
        "    # Extract article URLs\n",
        "    article_urls = extract_article_urls(url)\n",
        "\n",
        "    # Print extracted article URLs\n",
        "    print(\"Article URLs:\")\n",
        "    for article_url in article_urls:\n",
        "        #print(article_url)\n",
        "        scrape_newspaper(article_url)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "G3UC5YWZdute"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "\n",
        "def extract_article_urls(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser', from_encoding='iso-8859-7')\n",
        "\n",
        "        # Find all links on the page\n",
        "        links = soup.find_all('a', href=True)\n",
        "\n",
        "        # Filter out article URLs\n",
        "        article_urls = [link['href'] for link in links if is_article_url(link['href'])]\n",
        "\n",
        "        return article_urls\n",
        "    else:\n",
        "        print(\"Failed to retrieve webpage. Status code:\", response.status_code)\n",
        "        return []\n",
        "\n",
        "def is_article_url(url):\n",
        "    # Check if the URL is likely to be an article URL\n",
        "    # You may need to customize this function based on the structure of the website\n",
        "    # For Greek websites, you might look for patterns like \"/ειδήσεις/\" or \"/άρθρο/\"\n",
        "    return \"https://www.kathimerini.gr/\" in url\n",
        "\n",
        "def scrape_newspaper(url, output_file):\n",
        "    # Create Article object\n",
        "    article = Article(url)\n",
        "\n",
        "    # Download the article\n",
        "    article.download()\n",
        "\n",
        "    # Parse the article\n",
        "    article.parse()\n",
        "\n",
        "    # Write article text to the file in append mode\n",
        "    with open(output_file, 'a', encoding='utf-8') as f:\n",
        "        f.write(\"URL: {}\\n\".format(url))\n",
        "        f.write(article.text)\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "    print(\"Article text has been appended to\", output_file)\n",
        "\n",
        "def main():\n",
        "    output_file = \"kathimerini.txt\"\n",
        "    # URL of the Greek news website\n",
        "    url = \"https://www.kathimerini.gr/\"\n",
        "\n",
        "    # Extract article URLs\n",
        "    article_urls = extract_article_urls(url)\n",
        "\n",
        "    # Print extracted article URLs\n",
        "    print(\"Article URLs:\")\n",
        "    for article_url in article_urls:\n",
        "        scrape_newspaper(article_url, output_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "HTT8PB7wz6jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "\n",
        "def scrape_newspaper(urls, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for url in urls:\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            f.write(\"URL: {}\\n\".format(url))\n",
        "            f.write(article.text)\n",
        "            f.write(\"\\n\\n\")\n",
        "    print(\"Article texts have been written to\", output_file)\n",
        "\n",
        "def main():\n",
        "    # List of URLs of the articles you want to scrape\n",
        "    urls = [\n",
        "        \"https://nova.gr/etairia/deltia-typou/h-megalh-giorth-tou-gallofwnou-kinhmatografou-sthn-ellada-me-thn-sthriksh-ths-nova\",\n",
        "        \"https://nova.gr/etairia/deltia-typou/hello-thessalonikh-to-diktyo-optikwn-inwn-nova-fiber-eftase-kai-sth-thessalonikh\",\n",
        "        \"https://nova.gr/etairia/deltia-typou/nea-oikogeneiaka-programmata-nova-family-megalyterh-oikonomia-gia-olh-thn-oikogeneia\"\n",
        "    ]\n",
        "\n",
        "    # Name of the output file\n",
        "    output_file = \"articles.txt\"\n",
        "\n",
        "    # Call scrape_newspaper function with the list of URLs and output file name\n",
        "    scrape_newspaper(urls, output_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6Xy12PSznLG",
        "outputId": "425750f8-f88f-4b40-c317-be4893a993f4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article texts have been written to articles.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_article_urls(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser', from_encoding='iso-8859-7')\n",
        "\n",
        "        # Find all links on the page\n",
        "        links = soup.find_all('a', href=True)\n",
        "\n",
        "        # Filter out article URLs\n",
        "        article_urls = [link['href'] for link in links if is_article_url(link['href'])]\n",
        "\n",
        "        return article_urls\n",
        "    else:\n",
        "        print(\"Failed to retrieve webpage. Status code:\", response.status_code)\n",
        "        return []\n",
        "\n",
        "def is_article_url(url):\n",
        "    # Check if the URL is likely to be an article URL\n",
        "    # You may need to customize this function based on the structure of the website\n",
        "    # For Greek websites, you might look for patterns like \"/ειδήσεις/\" or \"/άρθρο/\"\n",
        "    return \"https://www.kathimerini.gr/\" in url\n",
        "\n",
        "def main():\n",
        "    output_file = \"kathimerini.txt\"\n",
        "    # URL of the Greek news website\n",
        "    url = \"https://www.kathimerini.gr/\"\n",
        "\n",
        "    # Extract article URLs\n",
        "    article_urls = extract_article_urls(url)\n",
        "\n",
        "    # Print extracted article URLs\n",
        "    print(\"Article URLs:\")\n",
        "    for article_url in article_urls:\n",
        "        #print(article_url)\n",
        "        scrape_newspaper(article_url,output_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "BGpb1TI-zP-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def extract_article_info(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser', from_encoding='iso-8859-7')\n",
        "\n",
        "        # Extract article title\n",
        "        title_element = soup.find('title')\n",
        "        title = title_element.text.strip() if title_element else \"Unknown\"\n",
        "\n",
        "        # Extract article author if available\n",
        "        author_element = soup.find('meta', attrs={'name': 'author'})\n",
        "        author = author_element['content'].strip() if author_element else \"Unknown\"\n",
        "\n",
        "        # Extract article publication date if available\n",
        "        date_element = soup.find('meta', attrs={'property': 'article:published_time'})\n",
        "        date = date_element['content'].strip() if date_element else \"Unknown\"\n",
        "\n",
        "        # Extract article text\n",
        "        article_body = soup.find('article')\n",
        "        text = article_body.text.strip() if article_body else \"Unknown\"\n",
        "\n",
        "        return title, author, date, text\n",
        "    else:\n",
        "        print(\"Failed to retrieve webpage. Status code:\", response.status_code)\n",
        "        return None, None, None, None\n",
        "\n",
        "def extract_article_urls(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser', from_encoding='iso-8859-7')\n",
        "\n",
        "        # Find all links on the page\n",
        "        links = soup.find_all('a', href=True)\n",
        "\n",
        "        # Filter out article URLs\n",
        "        article_urls = [link['href'] for link in links if is_article_url(link['href'])]\n",
        "\n",
        "        return article_urls\n",
        "    else:\n",
        "        print(\"Failed to retrieve webpage. Status code:\", response.status_code)\n",
        "        return []\n",
        "\n",
        "def is_article_url(url):\n",
        "    # Check if the URL is likely to be an article URL\n",
        "    # For Greek websites, you might look for patterns like \"/ειδήσεις/\" or \"/άρθρο/\"\n",
        "    #return \"/politics/\" in url or \"/life/technology/\" in url\n",
        "    return \"https://nova.gr/etairia/deltia-typou\" in url\n",
        "\n",
        "def main():\n",
        "    # URL of the Greek news website\n",
        "    url = \"https://nova.gr/etairia/deltia-typou\"\n",
        "\n",
        "    # Extract article URLs\n",
        "    article_urls = extract_article_urls(url)\n",
        "\n",
        "    # Write article information to CSV file\n",
        "    with open('articles.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['Title', 'Author', 'Date', 'Text']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Iterate through each article URL\n",
        "        for article_url in article_urls:\n",
        "            # Extract article information\n",
        "            title, author, date, text = extract_article_info(article_url)\n",
        "\n",
        "            # Write article information to CSV file\n",
        "            writer.writerow({'Title': title, 'Author': author, 'Date': date, 'Text': text})\n",
        "\n",
        "            print(\"Article saved:\", title)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "3rO4kG7gfBlm"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def csv_to_txt(csv_file, txt_file):\n",
        "    with open(csv_file, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        with open(txt_file, 'w', encoding='utf-8') as txtfile:\n",
        "            for row in reader:\n",
        "                txtfile.write(f\"Title: {row['Title']}\\n\")\n",
        "                txtfile.write(f\"Author: {row['Author']}\\n\")\n",
        "                txtfile.write(f\"Date: {row['Date']}\\n\")\n",
        "                txtfile.write(f\"Text: {row['Text']}\\n\\n\")\n",
        "\n",
        "# Example usage:\n",
        "csv_file = 'articles.csv'\n",
        "txt_file = 'articles.txt'\n",
        "csv_to_txt(csv_file, txt_file)\n"
      ],
      "metadata": {
        "id": "3MFlROpkjgKo"
      },
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}